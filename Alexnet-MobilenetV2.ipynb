{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06640917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalogramDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith('.npy'):\n",
    "                    self.data.append((os.path.join(class_dir, file), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data[idx]\n",
    "        scalogram = np.load(file_path).transpose(2, 0, 1)\n",
    "        scalogram = torch.tensor(scalogram, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            scalogram = self.transform(scalogram)\n",
    "        return scalogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e892d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "    mean = torch.zeros(2)\n",
    "    std = torch.zeros(2)\n",
    "    for inputs, _ in loader:\n",
    "        for i in range(2):\n",
    "            mean[i] += inputs[:, i].mean()\n",
    "            std[i] += inputs[:, i].std()\n",
    "    return (mean / len(loader)).tolist(), (std / len(loader)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualStreamCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Helper function to create a depthwise separable convolution block\n",
    "        def depthwise_separable_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "            return nn.Sequential(\n",
    "                # Depthwise convolution\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    in_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    groups=in_channels,  # Each input channel processed separately\n",
    "                    bias=False\n",
    "                ),\n",
    "                # Pointwise convolution (1x1 to change channel depth)\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            )\n",
    "\n",
    "        # Define reduced AlexNet stream with depthwise separable convolutions\n",
    "        def create_alexnet_stream():\n",
    "            return nn.Sequential(\n",
    "                # Block 1\n",
    "                depthwise_separable_conv(1, 16, kernel_size=11, stride=4, padding=2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                # Block 2\n",
    "                depthwise_separable_conv(16, 32, kernel_size=5, padding=2),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "                # Block 3\n",
    "                depthwise_separable_conv(32, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                # Block 4\n",
    "                depthwise_separable_conv(64, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                # Block 5\n",
    "                depthwise_separable_conv(64, 32, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            )\n",
    "\n",
    "        # Dual streams for separate channel processing\n",
    "        self.stream1 = create_alexnet_stream()\n",
    "        self.stream2 = create_alexnet_stream()\n",
    "\n",
    "        # Combined feature processing with depthwise separable convolutions\n",
    "        self.combined = nn.Sequential(\n",
    "            depthwise_separable_conv(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            depthwise_separable_conv(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Classifier remains unchanged\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process each channel separately\n",
    "        x1 = self.stream1(x[:, 0:1, :, :])  # First channel\n",
    "        x2 = self.stream2(x[:, 1:2, :, :])  # Second channel\n",
    "\n",
    "        # Combine feature maps\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        # Process combined features\n",
    "        x = self.combined(x)\n",
    "        return self.classifier(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "train_dir = 'Data/Splitted_Data/train'\n",
    "val_dir = 'Data/Splitted_Data/val'\n",
    "test_dir = 'Data/Splitted_Data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dataset stats\n",
    "temp_train = ScalogramDataset(train_dir)\n",
    "mean, std = calculate_dataset_stats(temp_train)\n",
    "print(f\"Dataset stats - Mean: {mean}, Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ScalogramDataset(train_dir, train_transform)\n",
    "val_dataset = ScalogramDataset(val_dir, val_test_transform)\n",
    "test_dataset = ScalogramDataset(test_dir, val_test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = DualStreamCNN(len(train_dataset.classes)).to(device)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f03511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Losses\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec192adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "patience = 7\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_acc = correct / total\n",
    "    val_acc = val_correct / val_total\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    if new_lr != current_lr:\n",
    "        print(f\"Learning rate reduced to {new_lr:.6f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss / len(train_loader):.4f} | Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss / len(val_loader):.4f} | Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"modelv5.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9633b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "model.load_state_dict(torch.load(\"modelv5.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
    "\n",
    "# Confusion matrices\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Raw counts\n",
    "plt.subplot(1, 2, 1)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=train_dataset.classes,\n",
    "            yticklabels=train_dataset.classes)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrices_modelv5.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
